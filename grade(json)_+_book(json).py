# -*- coding: utf-8 -*-
"""GRADE(json) + BOOK(json).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13ZeDrxa67GoAUke9iKYrtElemAN7TYu5
"""

from bs4 import BeautifulSoup
import requests

url = "https://raw.githubusercontent.com/ozlerhakan/mongodb-json-files/master/datasets/grades.json"
req = requests.get(url)

soup = BeautifulSoup(req.content)

soup

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re

df = pd.read_json("https://raw.githubusercontent.com/ozlerhakan/mongodb-json-files/master/datasets/grades.json",lines=True)

df

df.info()

def new_1(val1):
  return val1[0]['score']
df['exam_score']=df['scores'].apply(new_1)
def new_2(val2):
  return val2[1]['score']
df['quiz_score']=df['scores'].apply(new_2)
def new_3(val3):
  one = val3[-3]['score']
  two = val3[-2]['score']
  three = val3[-1]['score']
  return round((one+two+three)/3,2)
df['homework_score']=df['scores'].apply(new_3)

for i in range(df.shape[0]):
  new_id = dict(df.loc[i,'_id']).values()
  df.loc[i,"_id"] = new_id

df

df['scores'][0]

# exam_scores = []
# for i in range(len(df['scores'])):
#   exam_scores.append(re.sub('^{.*: |}','',str(df['scores'][i][0])))

exam_score = [round(float(re.sub('^{.*: |}','',str(df['scores'][i][0]))),2)for i in range(len(df['scores']))]
quiz_score = [round(float(re.sub('^{.*: |}','',str(df['scores'][i][1]))),2)for i in range(len(df['scores']))]
homework_score = [round(float(re.sub('^{.*: |}','',str(df['scores'][i][2]))),2)for i in range(len(df['scores']))]

exam_score

quiz_score

homework_score

for i in range(len(df['scores'])):
  li = re.findall('\d+.\d+',str(df['scores'][i][2:]))
  print(sum(list(map(float,li)))/len(list(map(float,li))))        # string to integer   # mean

df.head(1)

new_df = pd.DataFrame({
    "id": df['_id'],
    "student_id": df['student_id'],
    "class_id": df['class_id'],
    "exam_score": df['exam_score'],
    "quiz_score": df['quiz_score'],
    "homework_score": df['homework_score']
})

new_df

new_df.drop(['student_id'],inplace=True,axis=1)

new_df

x = new_df.exam_score[20:30]
y = new_df.homework_score[20:30]
plt.xlabel("Exam Score")
plt.ylabel("Homework Score")
plt.bar(x,y)

x = new_df.exam_score
y = new_df.homework_score
plt.scatter(x,y)
plt.xlabel("Exam Score")
plt.ylabel("Homework Score")
plt.title("Exam score vs Homework score")
plt.show()

"""BOOKS.JSON"""



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re

df = pd.read_json("https://raw.githubusercontent.com/ozlerhakan/mongodb-json-files/master/datasets/books.json",lines=True)
df

df.columns

df.columns.value_counts()

df.shape[1]

df.describe()

df.isnull().sum()

df.info()

df.tail(1)

re.sub('[A-Za-z]*|\n|-+|,+|:+','',str(df['isbn']))

df[df.isbn.isna()]

df.drop('isbn',axis=1,inplace=True)

df[df.title.duplicated()]

# deleting the duplicate values
df.drop_duplicates('title',inplace=True)

# df[df['title']=="Jaguar Development with PowerBuilder 7"]

df.info()

# rearrange the value of id column
df['_id']=np.arange(1,428)

df.info()

# reindexing
df=df.reset_index(drop=True)

df.info()

df.tail(2)

# replace nan values in pagecount column with meanof pagecount column
mean = round(np.mean(df.pageCount))
df['pageCount']=df['pageCount'].replace(0,mean)

df.info()

df

data = df.copy()

data.info()

data.publishedDate.fillna("0",inplace=True)

for i in range(len(data.publishedYear)):
  data.publishedYear[i]=re.sub("^{.*: '|-.*}","",str(data['publishedYear'][i]))

data.head(2)

# data.publishedYear.fillna(round(data.publishedYear.mean))
data.rename(columns={'publishedDate': 'publishedYear'}, inplace=True)

df.info()

df.rename(columns={'publishedDate': 'publishedYear'}, inplace=True)
print(df.columns)

data.info()

df.info()

df[df.publishedYear.isna()]

df.loc[[96]]

data.publishedYear.loc[[96]]

data.publishedYear = data.publishedYear.astype(int)

data.info()

mean = round(np.mean(data.publishedYear))
data['publishedYear']=data['publishedYear'].replace(0,mean)

# data.publishedYear.loc[[96]]

data.drop(['thumbnailUrl','shortDescription','longDescription'],axis=1,inplace=True)

data

data.status.unique()

data['status'].value_counts()

# MEAP to UNPUBLISH
data['status']=data['status'].replace('MEAP','UNPUBLISH')

data['status'].value_counts()

data.authors

# Drop _id, authors, categories columns

data.drop(['_id','authors','categories'],axis = 1,inplace = True)

data

data.info()

"""DATA ANALYSIS"""

data.head()

data['status'].value_counts().plot.bar(figsize=(10,5))
plt.grid()

# data['publishedYear'].value_counts()
# low = 1645
# high = 2014

# 1600 - 1700
# 1700 - 1800
# 1800 - 1900
# 1900 - 2000
# 2000 - 2100
data['publishedYear'].value_counts().plot.hist(figsize=(10,10), bins=5)

x=data['publishedYear'].value_counts()
plt.hist(x,bins=5)

data['publishedYear'].value_counts()

data

# List out the top 10 books that has the highest number of pages

data.sort_values('pageCount',ascending=False).head(10)

# List out the top 10 books that has the lowest number of pages

data.sort_values('pageCount').head(10)

data.head(10)

x=data.head(10)['title']
y=data.head(10)['pageCount']
plt.bar(x,y)

x = data.title.head(10)
y=data.pageCount.head(10)
plt.bar(x,y)

x = data.title.head(10)
y = data.pageCount.head(10)
plt.bar(x,y)
plt.show()

